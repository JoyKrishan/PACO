{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import sys, os\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "project_root = os.getenv('PROJECT_ROOT')\n",
    "sys.path.insert(0, project_root) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"runs/small_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prepare_data_for_models import build_train_val_test_iter\n",
    "from src.supconloss_with_cosine import SupConLossWithConsine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_dir = os.environ.get(\"DATA_DIR\")\n",
    "dataset_fullpath = os.path.join(project_root, data_dir, \"output\")\n",
    "small_dataset_output = os.path.join(dataset_fullpath, \"model_output\", \"small_patches\")\n",
    "large_dataset_output = os.path.join(dataset_fullpath, \"model_output\", \"large_patches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe\n",
    "large_patches_df = pd.read_csv(os.path.join(dataset_fullpath, \"large-patches.csv\"))\n",
    "small_patches_df = pd.read_csv(os.path.join(dataset_fullpath, \"small-patches.csv\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,3\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Building train, val and test csv files at /student/tdy245/Projects/cmpt828_deeplearning/PaCo/data/output/model_output/small_patches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Building train, val and test tabular dataset\n",
      "** Number of training examples: 946\n",
      "** Number of validation examples: 118\n",
      "** Number of testing examples: 119\n",
      "Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# small dataset\n",
    "train_itr_small, val_itr_small, test_itr_small, tokenizer = build_train_val_test_iter(small_dataset_output, small_patches_df, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMCodeEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.batch_norm_one = nn.BatchNorm1d(embedding_dim) \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.batch_norm_two = nn.BatchNorm1d(hidden_dim) \n",
    "        self.linear_projection = nn.Linear(hidden_dim, 128) \n",
    "        \n",
    "    def forward(self, code_tokens):\n",
    "        embeddings = self.embedding(code_tokens)\n",
    "        embeddings = self.batch_norm_one(embeddings) # Apply batch norm one\n",
    "        output, (hidden, cell) = self.lstm(embeddings)  \n",
    "        output = self.linear_projection(output)\n",
    "        # hidden = self.batch_norm_two(hidden[-1])  # Apply batch norm two\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_train_var(model):\n",
    "    criterion = SupConLossWithConsine(device=device)\n",
    "    optimizer =  optim.Adam(model.parameters(), lr= 1e-5)\n",
    "    return criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iterator, valid_iterator, batch_size):\n",
    "    nb_epochs = 100\n",
    "    criterion, optimizer = init_train_var(model)\n",
    "\n",
    "    train_steps = len(train_iterator.dataset) // batch_size\n",
    "    val_steps = len(valid_iterator.dataset) // batch_size\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(nb_epochs):\n",
    "        epoch_train_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "\n",
    "        # train_correct = 0  not using it now since there is no patch classifier\n",
    "        # val_correct = 0\n",
    "\n",
    "        for batch in train_iterator:\n",
    "            buggy_tensor, patch_tensor, labels = batch.buggy.T, batch.patch.T, batch.numerical_label # data already in device\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            buggy_embd = model(buggy_tensor)\n",
    "            patch_embd = model(patch_tensor)\n",
    "            loss = criterion(buggy_embd, patch_embd, labels) # buggy_embd, patch_embd, label\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss += loss\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_iterator:\n",
    "                buggy_tensor, patch_tensor, labels = batch.buggy.T, batch.patch.T, batch.numerical_label\n",
    "                buggy_embd = model(buggy_tensor)\n",
    "                patch_embd = model(patch_tensor)\n",
    "\n",
    "                loss = criterion(buggy_embd, patch_embd, labels) # buggy_embd, patch_embd, label\n",
    "                \n",
    "                epoch_val_loss += loss\n",
    "\n",
    "        mean_train_loss = epoch_train_loss / train_steps\n",
    "        mean_val_loss = epoch_val_loss / val_steps\n",
    "\n",
    "        writer.add_scalar(\"training_loss\", mean_train_loss, nb_epochs * train_steps + epoch)\n",
    "        writer.add_scalar(\"validation_loss\", mean_val_loss, nb_epochs * val_steps + epoch)\n",
    "        \n",
    "        print(\"[INFO] EPOCH: {}/{}\".format(epoch + 1, nb_epochs))\n",
    "        print(\"Train loss: {:.6f}\".format(mean_train_loss))\n",
    "        print(\"Val loss: {:.6f}\\n\".format(mean_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 1/100\n",
      "Train loss: 2.093079\n",
      "Val loss: 2.672560\n",
      "\n",
      "[INFO] EPOCH: 2/100\n",
      "Train loss: 2.088153\n",
      "Val loss: 2.666965\n",
      "\n",
      "[INFO] EPOCH: 3/100\n",
      "Train loss: 2.081014\n",
      "Val loss: 2.654811\n",
      "\n",
      "[INFO] EPOCH: 4/100\n",
      "Train loss: 2.055302\n",
      "Val loss: 2.513335\n",
      "\n",
      "[INFO] EPOCH: 5/100\n",
      "Train loss: 1.952000\n",
      "Val loss: 2.625548\n",
      "\n",
      "[INFO] EPOCH: 6/100\n",
      "Train loss: 2.017523\n",
      "Val loss: 2.519709\n",
      "\n",
      "[INFO] EPOCH: 7/100\n",
      "Train loss: 2.025662\n",
      "Val loss: 2.677988\n",
      "\n",
      "[INFO] EPOCH: 8/100\n",
      "Train loss: 2.091460\n",
      "Val loss: 2.673651\n",
      "\n",
      "[INFO] EPOCH: 9/100\n",
      "Train loss: 2.089927\n",
      "Val loss: 2.666314\n",
      "\n",
      "[INFO] EPOCH: 10/100\n",
      "Train loss: 2.078951\n",
      "Val loss: 2.654359\n",
      "\n",
      "[INFO] EPOCH: 11/100\n",
      "Train loss: 2.069102\n",
      "Val loss: 2.636626\n",
      "\n",
      "[INFO] EPOCH: 12/100\n",
      "Train loss: 2.050417\n",
      "Val loss: 2.604529\n",
      "\n",
      "[INFO] EPOCH: 13/100\n",
      "Train loss: 2.002002\n",
      "Val loss: 2.442308\n",
      "\n",
      "[INFO] EPOCH: 14/100\n",
      "Train loss: 2.011471\n",
      "Val loss: 2.609372\n",
      "\n",
      "[INFO] EPOCH: 15/100\n",
      "Train loss: 2.037206\n",
      "Val loss: 2.578414\n",
      "\n",
      "[INFO] EPOCH: 16/100\n",
      "Train loss: 1.969822\n",
      "Val loss: 2.365319\n",
      "\n",
      "[INFO] EPOCH: 17/100\n",
      "Train loss: 1.976095\n",
      "Val loss: 2.670962\n",
      "\n",
      "[INFO] EPOCH: 18/100\n",
      "Train loss: 2.085962\n",
      "Val loss: 2.667499\n",
      "\n",
      "[INFO] EPOCH: 19/100\n",
      "Train loss: 2.081731\n",
      "Val loss: 2.663735\n",
      "\n",
      "[INFO] EPOCH: 20/100\n",
      "Train loss: 2.077265\n",
      "Val loss: 2.659540\n",
      "\n",
      "[INFO] EPOCH: 21/100\n",
      "Train loss: 2.073400\n",
      "Val loss: 2.654510\n",
      "\n",
      "[INFO] EPOCH: 22/100\n",
      "Train loss: 2.071764\n",
      "Val loss: 2.647713\n",
      "\n",
      "[INFO] EPOCH: 23/100\n",
      "Train loss: 2.065517\n",
      "Val loss: 2.637662\n",
      "\n",
      "[INFO] EPOCH: 24/100\n",
      "Train loss: 2.056018\n",
      "Val loss: 2.617124\n",
      "\n",
      "[INFO] EPOCH: 25/100\n",
      "Train loss: 2.029799\n",
      "Val loss: 2.541133\n",
      "\n",
      "[INFO] EPOCH: 26/100\n",
      "Train loss: 2.020329\n",
      "Val loss: 2.654450\n",
      "\n",
      "[INFO] EPOCH: 27/100\n",
      "Train loss: 2.062770\n",
      "Val loss: 2.628665\n",
      "\n",
      "[INFO] EPOCH: 28/100\n",
      "Train loss: 2.048463\n",
      "Val loss: 2.609960\n",
      "\n",
      "[INFO] EPOCH: 29/100\n",
      "Train loss: 2.025470\n",
      "Val loss: 2.517216\n",
      "\n",
      "[INFO] EPOCH: 30/100\n",
      "Train loss: 2.066125\n",
      "Val loss: 2.666233\n",
      "\n",
      "[INFO] EPOCH: 31/100\n",
      "Train loss: 2.082767\n",
      "Val loss: 2.659768\n",
      "\n",
      "[INFO] EPOCH: 32/100\n",
      "Train loss: 2.078274\n",
      "Val loss: 2.650169\n",
      "\n",
      "[INFO] EPOCH: 33/100\n",
      "Train loss: 2.064014\n",
      "Val loss: 2.638601\n",
      "\n",
      "[INFO] EPOCH: 34/100\n",
      "Train loss: 2.058942\n",
      "Val loss: 2.627631\n",
      "\n",
      "[INFO] EPOCH: 35/100\n",
      "Train loss: 2.049472\n",
      "Val loss: 2.614805\n",
      "\n",
      "[INFO] EPOCH: 36/100\n",
      "Train loss: 2.029376\n",
      "Val loss: 2.584177\n",
      "\n",
      "[INFO] EPOCH: 37/100\n",
      "Train loss: 2.002890\n",
      "Val loss: 2.589944\n",
      "\n",
      "[INFO] EPOCH: 38/100\n",
      "Train loss: 2.004975\n",
      "Val loss: 2.499888\n",
      "\n",
      "[INFO] EPOCH: 39/100\n",
      "Train loss: 2.038342\n",
      "Val loss: 2.651528\n",
      "\n",
      "[INFO] EPOCH: 40/100\n",
      "Train loss: 2.049560\n",
      "Val loss: 2.592988\n",
      "\n",
      "[INFO] EPOCH: 41/100\n",
      "Train loss: 2.009435\n",
      "Val loss: 2.517713\n",
      "\n",
      "[INFO] EPOCH: 42/100\n",
      "Train loss: 1.958471\n",
      "Val loss: 2.491342\n",
      "\n",
      "[INFO] EPOCH: 43/100\n",
      "Train loss: 1.927469\n",
      "Val loss: 2.612490\n",
      "\n",
      "[INFO] EPOCH: 44/100\n",
      "Train loss: 2.014795\n",
      "Val loss: 2.511966\n",
      "\n",
      "[INFO] EPOCH: 45/100\n",
      "Train loss: 1.978747\n",
      "Val loss: 2.674018\n",
      "\n",
      "[INFO] EPOCH: 46/100\n",
      "Train loss: 2.092416\n",
      "Val loss: 2.669888\n",
      "\n",
      "[INFO] EPOCH: 47/100\n",
      "Train loss: 2.087209\n",
      "Val loss: 2.665395\n",
      "\n",
      "[INFO] EPOCH: 48/100\n",
      "Train loss: 2.081299\n",
      "Val loss: 2.660757\n",
      "\n",
      "[INFO] EPOCH: 49/100\n",
      "Train loss: 2.078700\n",
      "Val loss: 2.655749\n",
      "\n",
      "[INFO] EPOCH: 50/100\n",
      "Train loss: 2.072371\n",
      "Val loss: 2.649682\n",
      "\n",
      "[INFO] EPOCH: 51/100\n",
      "Train loss: 2.069824\n",
      "Val loss: 2.641093\n",
      "\n",
      "[INFO] EPOCH: 52/100\n",
      "Train loss: 2.060331\n",
      "Val loss: 2.627459\n",
      "\n",
      "[INFO] EPOCH: 53/100\n",
      "Train loss: 2.046852\n",
      "Val loss: 2.596110\n",
      "\n",
      "[INFO] EPOCH: 54/100\n",
      "Train loss: 2.005761\n",
      "Val loss: 2.475626\n",
      "\n",
      "[INFO] EPOCH: 55/100\n",
      "Train loss: 1.994216\n",
      "Val loss: 2.572366\n",
      "\n",
      "[INFO] EPOCH: 56/100\n",
      "Train loss: 1.969155\n",
      "Val loss: 2.606749\n",
      "\n",
      "[INFO] EPOCH: 57/100\n",
      "Train loss: 1.968246\n",
      "Val loss: 2.496078\n",
      "\n",
      "[INFO] EPOCH: 58/100\n",
      "Train loss: 1.976451\n",
      "Val loss: 2.211673\n",
      "\n",
      "[INFO] EPOCH: 59/100\n",
      "Train loss: 2.056196\n",
      "Val loss: 2.657412\n",
      "\n",
      "[INFO] EPOCH: 60/100\n",
      "Train loss: 2.072723\n",
      "Val loss: 2.648090\n",
      "\n",
      "[INFO] EPOCH: 61/100\n",
      "Train loss: 2.064004\n",
      "Val loss: 2.626648\n",
      "\n",
      "[INFO] EPOCH: 62/100\n",
      "Train loss: 2.028135\n",
      "Val loss: 2.517939\n",
      "\n",
      "[INFO] EPOCH: 63/100\n",
      "Train loss: 2.002873\n",
      "Val loss: 2.672970\n",
      "\n",
      "[INFO] EPOCH: 64/100\n",
      "Train loss: 2.094376\n",
      "Val loss: 2.671368\n",
      "\n",
      "[INFO] EPOCH: 65/100\n",
      "Train loss: 2.092179\n",
      "Val loss: 2.669225\n",
      "\n",
      "[INFO] EPOCH: 66/100\n",
      "Train loss: 2.089076\n",
      "Val loss: 2.667424\n",
      "\n",
      "[INFO] EPOCH: 67/100\n",
      "Train loss: 2.085824\n",
      "Val loss: 2.665887\n",
      "\n",
      "[INFO] EPOCH: 68/100\n",
      "Train loss: 2.082994\n",
      "Val loss: 2.664408\n",
      "\n",
      "[INFO] EPOCH: 69/100\n",
      "Train loss: 2.082417\n",
      "Val loss: 2.662840\n",
      "\n",
      "[INFO] EPOCH: 70/100\n",
      "Train loss: 2.079306\n",
      "Val loss: 2.661079\n",
      "\n",
      "[INFO] EPOCH: 71/100\n",
      "Train loss: 2.077460\n",
      "Val loss: 2.659050\n",
      "\n",
      "[INFO] EPOCH: 72/100\n",
      "Train loss: 2.075105\n",
      "Val loss: 2.656379\n",
      "\n",
      "[INFO] EPOCH: 73/100\n",
      "Train loss: 2.073430\n",
      "Val loss: 2.652694\n",
      "\n",
      "[INFO] EPOCH: 74/100\n",
      "Train loss: 2.069865\n",
      "Val loss: 2.646630\n",
      "\n",
      "[INFO] EPOCH: 75/100\n",
      "Train loss: 2.058735\n",
      "Val loss: 2.633174\n",
      "\n",
      "[INFO] EPOCH: 76/100\n",
      "Train loss: 2.033058\n",
      "Val loss: 2.504619\n",
      "\n",
      "[INFO] EPOCH: 77/100\n",
      "Train loss: 2.027437\n",
      "Val loss: 2.642068\n",
      "\n",
      "[INFO] EPOCH: 78/100\n",
      "Train loss: 2.060764\n",
      "Val loss: 2.625962\n",
      "\n",
      "[INFO] EPOCH: 79/100\n",
      "Train loss: 2.037717\n",
      "Val loss: 2.565972\n",
      "\n",
      "[INFO] EPOCH: 80/100\n",
      "Train loss: 1.966033\n",
      "Val loss: 2.576542\n",
      "\n",
      "[INFO] EPOCH: 81/100\n",
      "Train loss: 1.966775\n",
      "Val loss: 2.640763\n",
      "\n",
      "[INFO] EPOCH: 82/100\n",
      "Train loss: 2.056187\n",
      "Val loss: 2.613972\n",
      "\n",
      "[INFO] EPOCH: 83/100\n",
      "Train loss: 1.986750\n",
      "Val loss: 2.610398\n",
      "\n",
      "[INFO] EPOCH: 84/100\n",
      "Train loss: 2.023684\n",
      "Val loss: 2.626650\n",
      "\n",
      "[INFO] EPOCH: 85/100\n",
      "Train loss: 2.051646\n",
      "Val loss: 2.608571\n",
      "\n",
      "[INFO] EPOCH: 86/100\n",
      "Train loss: 2.034537\n",
      "Val loss: 2.577813\n",
      "\n",
      "[INFO] EPOCH: 87/100\n",
      "Train loss: 1.999071\n",
      "Val loss: 2.469336\n",
      "\n",
      "[INFO] EPOCH: 88/100\n",
      "Train loss: 1.856840\n",
      "Val loss: 2.513217\n",
      "\n",
      "[INFO] EPOCH: 89/100\n",
      "Train loss: 1.959389\n",
      "Val loss: 2.336503\n",
      "\n",
      "[INFO] EPOCH: 90/100\n",
      "Train loss: 1.976464\n",
      "Val loss: 2.644365\n",
      "\n",
      "[INFO] EPOCH: 91/100\n",
      "Train loss: 2.060180\n",
      "Val loss: 2.621896\n",
      "\n",
      "[INFO] EPOCH: 92/100\n",
      "Train loss: 2.037334\n",
      "Val loss: 2.583320\n",
      "\n",
      "[INFO] EPOCH: 93/100\n",
      "Train loss: 1.965260\n",
      "Val loss: 2.620648\n",
      "\n",
      "[INFO] EPOCH: 94/100\n",
      "Train loss: 2.061642\n",
      "Val loss: 2.631991\n",
      "\n",
      "[INFO] EPOCH: 95/100\n",
      "Train loss: 2.057754\n",
      "Val loss: 2.627061\n",
      "\n",
      "[INFO] EPOCH: 96/100\n",
      "Train loss: 2.056409\n",
      "Val loss: 2.619817\n",
      "\n",
      "[INFO] EPOCH: 97/100\n",
      "Train loss: 2.044998\n",
      "Val loss: 2.606392\n",
      "\n",
      "[INFO] EPOCH: 98/100\n",
      "Train loss: 2.027667\n",
      "Val loss: 2.564675\n",
      "\n",
      "[INFO] EPOCH: 99/100\n",
      "Train loss: 1.974165\n",
      "Val loss: 2.486600\n",
      "\n",
      "[INFO] EPOCH: 100/100\n",
      "Train loss: 1.959007\n",
      "Val loss: 2.378714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "code_encoder = LSTMCodeEncoder(tokenizer.vocab_size, 512, 512, num_layers=1)\n",
    "code_encoder.to(device)\n",
    "train(code_encoder, train_itr_small, val_itr_small, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
