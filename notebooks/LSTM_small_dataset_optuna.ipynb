{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model for the small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import sys, os\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "project_root = os.getenv('PROJECT_ROOT')\n",
    "sys.path.insert(0, project_root) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prepare_data_for_models import build_train_val_test_iter\n",
    "from src.supconloss_with_cosine import SupConLossWithConsine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42  # Choose an integer for your seed\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_dir = os.environ.get(\"DATA_DIR\")\n",
    "dataset_fullpath = os.path.join(project_root, data_dir, \"output\")\n",
    "small_dataset_output = os.path.join(dataset_fullpath, \"model_output\", \"small_patches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe\n",
    "small_patches_df = pd.read_csv(os.path.join(dataset_fullpath, \"small-patches.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Building train, val and test csv files at /student/tdy245/Projects/cmpt828_deeplearning/PaCo/data/output/model_output/small_patches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Building train, val and test tabular dataset\n",
      "** Number of training examples: 946\n",
      "** Number of validation examples: 118\n",
      "** Number of testing examples: 119\n",
      "Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# small dataset\n",
    "train_itr_small, val_itr_small, test_itr_small, tokenizer = build_train_val_test_iter(small_dataset_output, small_patches_df, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMCodeEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.batch_norm_one = nn.BatchNorm1d(embedding_dim) \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.batch_norm_two = nn.BatchNorm1d(hidden_dim) \n",
    "        self.linear_projection = nn.Linear(hidden_dim, 128) \n",
    "        \n",
    "    def forward(self, code_tokens):\n",
    "        embeddings = self.embedding(code_tokens)\n",
    "        embeddings = self.batch_norm_one(embeddings) # Apply batch norm one\n",
    "        output, (hidden, cell) = self.lstm(embeddings)  \n",
    "        output = self.linear_projection(output)\n",
    "        # hidden = self.batch_norm_two(hidden[-1])  # Apply batch norm two\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not using the function for now\n",
    "def init_train_var(model, trial):\n",
    "    criterion = SupConLossWithConsine(device=device)\n",
    "    optimizer =  optim.Adam(model.parameters(), lr= 1e-4)\n",
    "    return criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer_path = \"runs/small_dataset/all_runs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iterator, valid_iterator, batch_size, device, trial):\n",
    "    nb_epochs = 100\n",
    "\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-6, 1e-2, log=True)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam'])\n",
    "\n",
    "    train_steps = len(train_iterator.dataset) // batch_size\n",
    "    val_steps = len(valid_iterator.dataset) // batch_size\n",
    "\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=learning_rate) \n",
    "    criterion = SupConLossWithConsine(device=device)\n",
    "    \n",
    "    writer = SummaryWriter(writer_path + f\"{trial.number}\")\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(nb_epochs):\n",
    "        epoch_train_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "\n",
    "        # train_correct = 0  not using it now since there is no patch classifier\n",
    "        # val_correct = 0\n",
    "\n",
    "        for batch in train_iterator:\n",
    "            buggy_tensor, patch_tensor, labels = batch.buggy.T, batch.patch.T, batch.numerical_label # data already in device\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            buggy_embd = model(buggy_tensor)\n",
    "            patch_embd = model(patch_tensor)\n",
    "            loss = criterion(buggy_embd, patch_embd, labels) # buggy_embd, patch_embd, label\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss += loss\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_iterator:\n",
    "                buggy_tensor, patch_tensor, labels = batch.buggy.T, batch.patch.T, batch.numerical_label\n",
    "                buggy_embd = model(buggy_tensor)\n",
    "                patch_embd = model(patch_tensor)\n",
    "\n",
    "                loss = criterion(buggy_embd, patch_embd, labels) # buggy_embd, patch_embd, label\n",
    "                \n",
    "                epoch_val_loss += loss\n",
    "\n",
    "        mean_train_loss = epoch_train_loss / train_steps\n",
    "        mean_val_loss = epoch_val_loss / val_steps\n",
    "\n",
    "        writer.add_scalar(\"training_loss\", mean_train_loss, epoch + 1)\n",
    "        writer.add_scalar(\"validation_loss\", mean_val_loss, epoch + 1)\n",
    "        \n",
    "        # print(\"[INFO] EPOCH: {}/{}\".format(epoch + 1, nb_epochs))\n",
    "        # print(\"Train loss: {:.6f}\".format(mean_train_loss))\n",
    "        # print(\"Val loss: {:.6f}\\n\".format(mean_val_loss))\n",
    "    \n",
    "    return mean_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    code_encoder = LSTMCodeEncoder(tokenizer.vocab_size, 512, 512, num_layers=1)\n",
    "    code_encoder.to(device)\n",
    "    val_loss = train(code_encoder, train_itr_small, val_itr_small, 32, device, trial) # bad practice will change later\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-26 20:03:02,424] A new study created in memory with name: no-name-4f70751d-b34e-4580-bb6b-9e4712c64ae4\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[I 2024-03-26 20:06:06,958] Trial 0 finished with value: 1.3675405979156494 and parameters: {'learning_rate': 1.417452143273518e-05, 'optimizer': 'Adam'}. Best is trial 0 with value: 1.3675405979156494.\n",
      "[I 2024-03-26 20:09:07,158] Trial 1 finished with value: 2.1669793128967285 and parameters: {'learning_rate': 1.1901264338270423e-05, 'optimizer': 'Adam'}. Best is trial 0 with value: 1.3675405979156494.\n",
      "[I 2024-03-26 20:12:07,586] Trial 2 finished with value: 2.603339672088623 and parameters: {'learning_rate': 2.1963478047538964e-06, 'optimizer': 'Adam'}. Best is trial 0 with value: 1.3675405979156494.\n",
      "[I 2024-03-26 20:14:52,661] Trial 3 finished with value: 2.675435781478882 and parameters: {'learning_rate': 1.2934903516274945e-05, 'optimizer': 'Adam'}. Best is trial 0 with value: 1.3675405979156494.\n",
      "[I 2024-03-26 20:17:24,347] Trial 4 finished with value: 1.3829859495162964 and parameters: {'learning_rate': 9.439504015070076e-05, 'optimizer': 'Adam'}. Best is trial 0 with value: 1.3675405979156494.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'optuna.study' has no attribute 'TrialState'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      5\u001b[0m optuna\u001b[38;5;241m.\u001b[39mTrialPruned\n\u001b[0;32m----> 6\u001b[0m pruned_trials \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m study\u001b[38;5;241m.\u001b[39mtrials \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m optuna\u001b[38;5;241m.\u001b[39mstudy\u001b[38;5;241m.\u001b[39mTrialState\u001b[38;5;241m.\u001b[39mPRUNED]\n\u001b[1;32m      7\u001b[0m complete_trials \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m study\u001b[38;5;241m.\u001b[39mtrials \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m optuna\u001b[38;5;241m.\u001b[39mstudy\u001b[38;5;241m.\u001b[39mTrialState\u001b[38;5;241m.\u001b[39mCOMPLETE]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStudy statistics: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      5\u001b[0m optuna\u001b[38;5;241m.\u001b[39mTrialPruned\n\u001b[0;32m----> 6\u001b[0m pruned_trials \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m study\u001b[38;5;241m.\u001b[39mtrials \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m \u001b[43moptuna\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrialState\u001b[49m\u001b[38;5;241m.\u001b[39mPRUNED]\n\u001b[1;32m      7\u001b[0m complete_trials \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m study\u001b[38;5;241m.\u001b[39mtrials \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m optuna\u001b[38;5;241m.\u001b[39mstudy\u001b[38;5;241m.\u001b[39mTrialState\u001b[38;5;241m.\u001b[39mCOMPLETE]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStudy statistics: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'optuna.study' has no attribute 'TrialState'"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction='minimize')  # Aim to minimize  validation loss\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "print(best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
