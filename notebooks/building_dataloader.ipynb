{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext.data import BucketIterator\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.environ.get(\"PROJECT_ROOT\")\n",
    "data_dir = os.environ.get(\"DATA_DIR\")\n",
    "dataset_fullpath = os.path.join(project_root, data_dir, \"output\")\n",
    "small_dataset_output = os.path.join(dataset_fullpath, \"model_output\", \"small_patches\")\n",
    "large_dataset_output = os.path.join(dataset_fullpath, \"model_output\", \"large_patches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_patches_df = pd.read_csv(os.path.join(dataset_fullpath, \"large-patches.csv\"))\n",
    "small_patches_df = pd.read_csv(os.path.join(dataset_fullpath, \"small-patches.csv\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing small patches dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbc8afb8290>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def csv_train_test_split(save_path, df):\n",
    "\n",
    "    df = df.sample(frac=1).reset_index(drop=True)  \n",
    "\n",
    "    df['numerical_label'] = df['label'].replace({'correct': 1, 'overfitting': -1})\n",
    "\n",
    "    total_rows = len(df)\n",
    "    first = int(0.8 * total_rows)\n",
    "    second = int(0.9 * total_rows)\n",
    "\n",
    "    train_df = df.iloc[:first]\n",
    "    val_df = df.iloc[first:second]\n",
    "    test_df = df.iloc[second:]\n",
    "\n",
    "    train_df.to_csv(os.path.join(save_path, \"train.csv\"), index=False)\n",
    "    val_df.to_csv(os.path.join(save_path, \"val.csv\"), index=False)\n",
    "    test_df.to_csv(os.path.join(save_path, \"test.csv\"), index=False)\n",
    "\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = csv_train_test_split(small_dataset_output, small_patches_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using codebert pretrained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained('microsoft/codebert-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "pad_index = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "unk_index = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset and Building the Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buggy # patch # label\n",
    "BUGGY_CODE = data.Field(use_vocab =False, tokenize = tokenizer.encode, pad_token=pad_index, unk_token=unk_index, fix_length=MAX_SEQ_LEN)\n",
    "PATCH_CODE = data.Field(use_vocab =False, tokenize= tokenizer.encode, pad_token=pad_index, unk_token=unk_index, fix_length=MAX_SEQ_LEN)\n",
    "LABEL = data.Field(use_vocab=False, sequential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [(\"dataset\", None),\n",
    "        (\"tool\", None),\n",
    "        (\"buggy\", BUGGY_CODE),\n",
    "        (\"patch\", PATCH_CODE),\n",
    "        (\"label\", None),\n",
    "        (\"numerical_label\", LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dataset', None),\n",
       " ('tool', None),\n",
       " ('buggy', <torchtext.data.field.Field at 0x7fbb2ea6e9e0>),\n",
       " ('patch', <torchtext.data.field.Field at 0x7fbb2ea6f190>),\n",
       " ('label', None),\n",
       " ('numerical_label', <torchtext.data.field.Field at 0x7fbb2ea6efe0>)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.TabularDataset(\n",
    "        path= os.path.join(small_dataset_output, \"train.csv\"),\n",
    "        format=\"csv\",\n",
    "        skip_header=True,\n",
    "        fields=fields\n",
    "        )\n",
    "val =  data.TabularDataset(\n",
    "        path= os.path.join(small_dataset_output, \"val.csv\"),\n",
    "        format=\"csv\",\n",
    "        skip_header=True,\n",
    "        fields=fields\n",
    "        )\n",
    "test = data.TabularDataset(\n",
    "        path= os.path.join(small_dataset_output, \"test.csv\"),\n",
    "        format=\"csv\",\n",
    "        skip_header=True,\n",
    "        fields=fields\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 946\n",
      "Number of validation examples: 118\n",
      "Number of testing examples: 119\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Number of training examples: {len(train.examples)}\")\n",
    "print(f\"Number of validation examples: {len(val.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'buggy': [0, 15110, 5997, 45200, 40462, 48176, 49162, 28874, 13984, 1640, 46674, 26907, 6, 6979, 22523, 6, 16224, 11212, 42379, 43, 25522, 114, 36, 36097, 8061, 321, 43, 25522, 1306, 15791, 18583, 1640, 10799, 2055, 22523, 4397, 26602, 7031, 5457, 36, 46134, 45994, 23796, 17487, 120, 49302, 39645, 43048, 4832, 26907, 4, 560, 34222, 49291, 6979, 7031, 45280, 5457, 7031, 4, 16096, 47006, 114, 36, 6031, 45280, 49095, 22523, 43, 25522, 1437, 2], 'patch': [0, 1290, 2407, 15791, 18583, 48461, 10799, 43, 2055, 204, 4397, 1437, 2], 'numerical_label': '-1'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train.examples[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train, val, test), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    sort_key = lambda x : len(x.__dict__),\n",
    "    sort_within_batch = False,\n",
    "    device = device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "buggy: tensor([[    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [46674,  1594,  1594,  ...,  3983,  1594, 15110],\n",
      "        [40118,    36,    36,  ..., 48547,    36, 26602],\n",
      "        ...,\n",
      "        [    1,     1,     1,  ...,     1,     1,     1],\n",
      "        [    1,     1,     1,  ...,     1,     1,     1],\n",
      "        [    1,     1,     1,  ...,     1,     1,     1]], device='cuda:2')\n",
      "buggy size: torch.Size([32, 512])\n",
      "patch: tensor([[    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [46674,  1594,  1594,  ...,  1594,  1594, 15117],\n",
      "        [40118,    36, 48461,  ..., 41006, 48614, 29702],\n",
      "        ...,\n",
      "        [    1,     1,     1,  ...,     1,     1,     1],\n",
      "        [    1,     1,     1,  ...,     1,     1,     1],\n",
      "        [    1,     1,     1,  ...,     1,     1,     1]], device='cuda:2')\n",
      "patch size: torch.Size([32, 512])\n",
      "label: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_iterator):\n",
    "    print(\"i:\",i)\n",
    "    one_bug_sample = batch.buggy\n",
    "    one_patch_sample = batch.patch\n",
    "    label = batch.numerical_label\n",
    "    print(f\"buggy: {batch.buggy}\")\n",
    "    print(f\"buggy size: {batch.buggy.T.size()}\")\n",
    "    print(f\"patch: {batch.patch}\")\n",
    "    print(f\"patch size: {batch.patch.T.size()}\")\n",
    "    print(f\"label: {batch.numerical_label.size()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "buggy: tensor([[    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [24303,  1594,  2544,  ..., 10339, 24303, 15110],\n",
      "        [  403,    36, 13561,  ...,   131,   940,   230],\n",
      "        ...,\n",
      "        [    1,     1,     1,  ...,     1,     1,     1],\n",
      "        [    1,     1,     1,  ...,     1,     1,     1],\n",
      "        [    1,     1,     1,  ...,     1,     1,     1]], device='cuda:2')\n",
      "buggy size: torch.Size([32, 512])\n",
      "patch: tensor([[    0,     0,     0,  ...,     0,     0,     0],\n",
      "        [ 1594,   180, 20235,  ...,  1594,     2,  1437],\n",
      "        [48461, 10643,    36,  ..., 48461,     1,     2],\n",
      "        ...,\n",
      "        [    1,     1,     1,  ...,     1,     1,     1],\n",
      "        [    1,     1,     1,  ...,     1,     1,     1],\n",
      "        [    1,     1,     1,  ...,     1,     1,     1]], device='cuda:2')\n",
      "patch size: torch.Size([32, 512])\n",
      "label: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(valid_iterator):\n",
    "    print(\"i:\",i)\n",
    "    one_bug_sample = batch.buggy\n",
    "    one_patch_sample = batch.patch\n",
    "    label = batch.numerical_label\n",
    "    print(f\"buggy: {batch.buggy}\")\n",
    "    print(f\"buggy size: {batch.buggy.T.size()}\")\n",
    "    print(f\"patch: {batch.patch}\")\n",
    "    print(f\"patch size: {batch.patch.T.size()}\")\n",
    "    print(f\"label: {batch.numerical_label.size()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'return',\n",
       " 'Ġread',\n",
       " 'Object',\n",
       " ';',\n",
       " 'Ġ}',\n",
       " 'Ġcatch',\n",
       " 'Ġ(',\n",
       " 'Class',\n",
       " 'Not',\n",
       " 'Found',\n",
       " 'Exception',\n",
       " 'Ġex',\n",
       " ')',\n",
       " 'Ġ{',\n",
       " 'Ġthrow',\n",
       " 'Ġnew',\n",
       " 'ĠSerial',\n",
       " 'ization',\n",
       " 'Exception',\n",
       " '(\"',\n",
       " 'Class',\n",
       " 'Not',\n",
       " 'Found',\n",
       " 'Exception',\n",
       " 'Ġwhile',\n",
       " 'Ġreading',\n",
       " 'Ġcl',\n",
       " 'oned',\n",
       " 'Ġobject',\n",
       " 'Ġdata',\n",
       " '\",',\n",
       " 'Ġex',\n",
       " ');',\n",
       " 'Ġ}',\n",
       " 'Ġcatch',\n",
       " 'Ġ(',\n",
       " 'IO',\n",
       " 'Exception',\n",
       " 'Ġex',\n",
       " ')',\n",
       " 'Ġ{',\n",
       " 'Ġthrow',\n",
       " 'Ġnew',\n",
       " 'ĠSerial',\n",
       " 'ization',\n",
       " 'Exception',\n",
       " '(\"',\n",
       " 'IO',\n",
       " 'Exception',\n",
       " 'Ġwhile',\n",
       " 'Ġreading',\n",
       " 'Ġcl',\n",
       " 'oned',\n",
       " 'Ġobject',\n",
       " 'Ġdata',\n",
       " '\",',\n",
       " 'Ġex',\n",
       " ');',\n",
       " 'Ġ}',\n",
       " 'Ġfinally',\n",
       " 'Ġ{',\n",
       " 'Ġ',\n",
       " '</s>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(one_bug_sample[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 50265\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer)\n",
    "\n",
    "print(\"Vocabulary Size:\", vocab_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class CodeEncoder(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim, num_heads, num_layers, dropout=0.1):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.pos_encoder = PositionalEncoding(d_model=embedding_dim)\n",
    "    self.embedding_dim = embedding_dim\n",
    "\n",
    "    self.transformer_encoder = nn.TransformerEncoder(\n",
    "        nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, nhead=num_heads, dim_feedforward=4 * embedding_dim, dropout=dropout\n",
    "        ),\n",
    "        num_layers=num_layers\n",
    "    )\n",
    "\n",
    "  def forward(self, code_input):\n",
    "    embedded_code = self.embedding(code_input) * math.sqrt(self.embedding_dim) \n",
    "    encoded_code = self.pos_encoder(embedded_code)\n",
    "    output = self.transformer_encoder(encoded_code)  \n",
    "  \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.\n",
    "    It also supports the unsupervised contrastive loss in SimCLR\"\"\"\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
    "                 base_temperature=0.07):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "        self.base_temperature = base_temperature\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device = (torch.device('cuda:2')\n",
    "                  if features.is_cuda\n",
    "                  else torch.device('cpu'))\n",
    "\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "        # compute mean of log-likelihood over positive\n",
    "        # modified to handle edge cases when there is no positive pair\n",
    "        # for an anchor point. \n",
    "        # Edge case e.g.:- \n",
    "        # features of shape: [4,1,...]\n",
    "        # labels:            [0,1,1,2]\n",
    "        # loss before mean:  [nan, ..., ..., nan] \n",
    "        mask_pos_pairs = mask.sum(1)\n",
    "        mask_pos_pairs = torch.where(mask_pos_pairs < 1e-6, 1, mask_pos_pairs)\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask_pos_pairs\n",
    "\n",
    "        # loss\n",
    "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_train_var(model):\n",
    "    criterion = SupConLoss()\n",
    "    optimizer =  optim.SGD(model.parameters(), lr= 0.01)\n",
    "    return criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iterator, valid_iterator, batch_size):\n",
    "    nb_epochs = 20\n",
    "    criterion, optimizer = init_train_var(model)\n",
    "\n",
    "    train_steps = len(train_iterator.dataset) // batch_size\n",
    "    val_steps = len(valid_iterator.dataset) // batch_size\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(nb_epochs):\n",
    "        epoch_train_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "\n",
    "        # train_correct = 0  not using it now since there is no patch classifier\n",
    "        # val_correct = 0\n",
    "\n",
    "        for batch in train_iterator:\n",
    "            buggy_tensor, patch_tensor, labels = batch.buggy.T, batch.patch.T, batch.numerical_label # data already in device\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            buggy_embd = model(buggy_tensor)\n",
    "            patch_embd = model(patch_tensor)\n",
    "\n",
    "            feature = torch.norm(buggy_embd - patch_embd, dim=2, keepdim=True)\n",
    "    \n",
    "            loss = criterion(feature, labels) # feature, label\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss += loss\n",
    "\n",
    "        print(epoch_train_loss)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_iterator:\n",
    "                buggy_tensor, patch_tensor, labels = batch.buggy.T, batch.patch.T, batch.numerical_label\n",
    "                buggy_embd = model(buggy_tensor)\n",
    "                patch_embd = model(patch_tensor)\n",
    "\n",
    "                feature = torch.norm(buggy_embd - patch_embd, dim=2, keepdim=True)\n",
    "                loss = criterion(feature, labels)\n",
    "\n",
    "                epoch_val_loss += loss\n",
    "\n",
    "        mean_train_loss = epoch_train_loss / train_steps\n",
    "        mean_val_loss = epoch_val_loss / val_steps\n",
    "\n",
    "        print(\"[INFO] EPOCH: {}/{}\".format(epoch + 1, nb_epochs))\n",
    "        print(\"Train loss: {:.6f}\".format(mean_train_loss))\n",
    "        print(\"Val loss: {:.6f}\\n\".format(mean_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4884.5879, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "[INFO] EPOCH: 1/20\n",
      "Train loss: 168.434067\n",
      "Val loss: 12.813759\n",
      "\n",
      "tensor(290.5446, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "[INFO] EPOCH: 2/20\n",
      "Train loss: 10.018778\n",
      "Val loss: 12.813759\n",
      "\n",
      "tensor(290.5446, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "[INFO] EPOCH: 3/20\n",
      "Train loss: 10.018778\n",
      "Val loss: 12.813759\n",
      "\n",
      "tensor(290.5445, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "[INFO] EPOCH: 4/20\n",
      "Train loss: 10.018777\n",
      "Val loss: 12.813759\n",
      "\n",
      "tensor(290.5445, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "[INFO] EPOCH: 5/20\n",
      "Train loss: 10.018777\n",
      "Val loss: 12.813759\n",
      "\n",
      "tensor(290.5446, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "[INFO] EPOCH: 6/20\n",
      "Train loss: 10.018778\n",
      "Val loss: 12.813759\n",
      "\n",
      "tensor(290.5446, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "[INFO] EPOCH: 7/20\n",
      "Train loss: 10.018778\n",
      "Val loss: 12.813759\n",
      "\n",
      "tensor(290.5446, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "[INFO] EPOCH: 8/20\n",
      "Train loss: 10.018778\n",
      "Val loss: 12.813759\n",
      "\n",
      "tensor(290.5446, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "[INFO] EPOCH: 9/20\n",
      "Train loss: 10.018778\n",
      "Val loss: 12.813759\n",
      "\n",
      "tensor(290.5446, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "[INFO] EPOCH: 10/20\n",
      "Train loss: 10.018778\n",
      "Val loss: 12.813759\n",
      "\n",
      "tensor(290.5446, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "[INFO] EPOCH: 11/20\n",
      "Train loss: 10.018778\n",
      "Val loss: 12.813759\n",
      "\n",
      "tensor(290.5446, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "[INFO] EPOCH: 12/20\n",
      "Train loss: 10.018778\n",
      "Val loss: 12.813759\n",
      "\n",
      "tensor(290.5446, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "[INFO] EPOCH: 13/20\n",
      "Train loss: 10.018778\n",
      "Val loss: 12.813759\n",
      "\n",
      "tensor(290.5446, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "[INFO] EPOCH: 14/20\n",
      "Train loss: 10.018778\n",
      "Val loss: 12.813759\n",
      "\n",
      "tensor(290.5446, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "[INFO] EPOCH: 15/20\n",
      "Train loss: 10.018778\n",
      "Val loss: 12.813759\n",
      "\n",
      "tensor(290.5446, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "[INFO] EPOCH: 16/20\n",
      "Train loss: 10.018778\n",
      "Val loss: 12.813759\n",
      "\n",
      "tensor(290.5445, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "[INFO] EPOCH: 17/20\n",
      "Train loss: 10.018777\n",
      "Val loss: 12.813759\n",
      "\n",
      "tensor(290.5446, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "[INFO] EPOCH: 18/20\n",
      "Train loss: 10.018778\n",
      "Val loss: 12.813759\n",
      "\n",
      "tensor(290.5446, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "[INFO] EPOCH: 19/20\n",
      "Train loss: 10.018778\n",
      "Val loss: 12.813759\n",
      "\n",
      "tensor(290.5446, device='cuda:2', grad_fn=<AddBackward0>)\n",
      "[INFO] EPOCH: 20/20\n",
      "Train loss: 10.018778\n",
      "Val loss: 12.813759\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code_encoder = CodeEncoder(vocab_size, embedding_dim=300, num_heads=4, num_layers=4)\n",
    "code_encoder.to(device)\n",
    "train(code_encoder, train_iterator, valid_iterator, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19415100"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in code_encoder.parameters())\n",
    "total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
