{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model for the large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import sys, os\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "project_root = os.getenv('PROJECT_ROOT')\n",
    "sys.path.insert(0, project_root) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prepare_data_for_models import build_train_val_test_iter\n",
    "from src.supconloss_with_cosine import SupConLossWithConsine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42  # Choose an integer for your seed\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_dir = os.environ.get(\"DATA_DIR\")\n",
    "dataset_fullpath = os.path.join(project_root, data_dir, \"output\")\n",
    "large_dataset_output = os.path.join(dataset_fullpath, \"model_output\", \"large_patches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_save_path = os.path.join(project_root, \"notebooks\", \"runs\", \"large_dataset\", \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe\n",
    "large_patches_df = pd.read_csv(os.path.join(dataset_fullpath, \"large-patches.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Building train, val and test csv files at /student/tdy245/Projects/cmpt828_deeplearning/PaCo/data/output/model_output/large_patches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (564 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Building train, val and test tabular dataset\n",
      "** Number of training examples: 39755\n",
      "** Number of validation examples: 4969\n",
      "** Number of testing examples: 4970\n",
      "Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# small dataset\n",
    "train_itr_large, val_itr_large, test_itr_large, tokenizer = build_train_val_test_iter(large_dataset_output, large_patches_df, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMCodeEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.batch_norm_one = nn.BatchNorm1d(embedding_dim) \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.batch_norm_two = nn.BatchNorm1d(hidden_dim) \n",
    "        self.linear_projection = nn.Linear(hidden_dim, 128) \n",
    "        \n",
    "    def forward(self, code_tokens):\n",
    "        embeddings = self.embedding(code_tokens)\n",
    "        embeddings = self.batch_norm_one(embeddings) # Apply batch norm one\n",
    "        output, (hidden, cell) = self.lstm(embeddings)  \n",
    "        output = self.linear_projection(output)\n",
    "        # hidden = self.batch_norm_two(hidden[-1])  # Apply batch norm two\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not using the function for now\n",
    "def init_train_var(model, trial):\n",
    "    criterion = SupConLossWithConsine(device=device)\n",
    "    optimizer =  optim.Adam(model.parameters(), lr= 1e-4)\n",
    "    return criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer_path = \"runs/large_dataset/all_runs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iterator, valid_iterator, batch_size, device, trial):\n",
    "    nb_epochs = 50\n",
    "\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-2)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam'])\n",
    "\n",
    "    train_steps = len(train_iterator.dataset) // batch_size\n",
    "    val_steps = len(valid_iterator.dataset) // batch_size\n",
    "\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=learning_rate) \n",
    "    criterion = SupConLossWithConsine(device=device)\n",
    "    \n",
    "    writer = SummaryWriter(writer_path + f\"{trial.number}\")\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(nb_epochs):\n",
    "        epoch_train_loss = 0\n",
    "        epoch_val_loss = 0\n",
    "\n",
    "        # train_correct = 0  not using it now since there is no patch classifier\n",
    "        # val_correct = 0\n",
    "\n",
    "        for batch in train_iterator:\n",
    "            buggy_tensor, patch_tensor, labels = batch.buggy.T, batch.patch.T, batch.numerical_label # data already in device\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            buggy_embd = model(buggy_tensor)\n",
    "            patch_embd = model(patch_tensor)\n",
    "            loss = criterion(buggy_embd, patch_embd, labels) # buggy_embd, patch_embd, label\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss += loss\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_iterator:\n",
    "                buggy_tensor, patch_tensor, labels = batch.buggy.T, batch.patch.T, batch.numerical_label \n",
    "                epoch_val_loss += loss\n",
    "\n",
    "        mean_train_loss = epoch_train_loss / train_steps\n",
    "        mean_val_loss = epoch_val_loss / val_steps\n",
    "\n",
    "        writer.add_scalar(\"training_loss\", mean_train_loss, epoch + 1)\n",
    "        writer.add_scalar(\"validation_loss\", mean_val_loss, epoch + 1)\n",
    "\n",
    "    writer.add_hparams({'lr': learning_rate}, {'train_loss': mean_train_loss, 'val_loss': mean_val_loss})\n",
    "        # print(\"[INFO] EPOCH: {}/{}\".format(epoch + 1, nb_epochs))\n",
    "        # print(\"Train loss: {:.6f}\".format(mean_train_loss))\n",
    "        # print(\"Val loss: {:.6f}\\n\".format(mean_val_loss))  \n",
    "    return mean_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def create_and_run_study(model_savepath, n_trials = 2):\n",
    "    study = optuna.create_study(direction='minimize')   # Aim to minimize  validation loss\n",
    "\n",
    "    def save_model(model, filename):\n",
    "        torch.save(model.state_dict(), os.path.join(model_savepath, filename))\n",
    "        \n",
    "    def objective(trial):\n",
    "        code_encoder = LSTMCodeEncoder(tokenizer.vocab_size, 512, 512, num_layers=1)\n",
    "        code_encoder.to(device)\n",
    "        val_loss = train(code_encoder, train_itr_large, val_itr_large, 32, device, trial) # bad practice will change later\n",
    "\n",
    "        if trial.number == 0 or val_loss < study.best_value:\n",
    "            save_model(code_encoder, \"best_model_small.pth\")\n",
    "        return val_loss  \n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-27 19:19:28,476] A new study created in memory with name: no-name-e2203408-b261-4dba-87cf-6a8d95accb20\n"
     ]
    }
   ],
   "source": [
    "create_and_run_study(best_model_save_path, n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
